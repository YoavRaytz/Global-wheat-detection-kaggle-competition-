{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fzy22yz8TwG"
   },
   "outputs": [],
   "source": [
    "### Utilize GPU if available\n",
    "!pip install \"torch==1.4\" \"torchvision==0.5.0\"\n",
    "\n",
    "import torch\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "!nvidia-smi\n",
    "torch.cuda.empty_cache()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IM1STyQjo0b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "z57ktbw1M_uy",
    "outputId": "035e35f3-f042-4f2a-ae6b-3c87ebe5617d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 1.5MB 8.1MB/s \n",
      "\u001b[?25hCollecting adamod\n",
      "  Downloading adamod-0.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adamod) (1.6.0+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adamod) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adamod) (1.18.5)\n",
      "Installing collected packages: adamod\n",
      "Successfully installed adamod-0.0.3\n",
      "\u001b[K     |████████████████████████████████| 948 kB 8.2 MB/s \n",
      "\u001b[?25h  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ensemble-boxes\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q six numpy scipy Pillow matplotlib scikit-image opencv-python imageio\n",
    "!pip install adamod\n",
    "!pip install -U -q git+https://github.com/albumentations-team/albumentations\n",
    "print('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "OeHA2KwsTPgI",
    "outputId": "4dd82dc5-d707-4c19-9b46-062d17dc36bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from albumentations import Normalize\n",
    "import adamod as adamod\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from ensemble_boxes import *\n",
    "from itertools import product\n",
    "import numba\n",
    "from numba import jit\n",
    "from typing import List, Union, Tuple\n",
    "from numba.typed import List\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNTcABSmKjNT"
   },
   "source": [
    "#load csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "AeTAVh4tS03n",
    "outputId": "f53ac84d-7917-4092-c4aa-0d86d08232c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x_start</th>\n",
       "      <th>y_start</th>\n",
       "      <th>x_end</th>\n",
       "      <th>y_end</th>\n",
       "      <th>area</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>226.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>7540.0</td>\n",
       "      <td>0.446154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>377.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>11840.0</td>\n",
       "      <td>2.162162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>11663.0</td>\n",
       "      <td>0.981651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>14508.0</td>\n",
       "      <td>0.943548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height   source  ...  x_end  y_end     area     ratio\n",
       "0  b6ab77fd7   1024    1024  usask_1  ...  890.0  258.0   2016.0  0.642857\n",
       "1  b6ab77fd7   1024    1024  usask_1  ...  356.0  606.0   7540.0  0.446154\n",
       "2  b6ab77fd7   1024    1024  usask_1  ...  451.0  664.0  11840.0  2.162162\n",
       "3  b6ab77fd7   1024    1024  usask_1  ...  943.0  202.0  11663.0  0.981651\n",
       "4  b6ab77fd7   1024    1024  usask_1  ...  150.0  261.0  14508.0  0.943548\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = '/content/gdrive/My Drive/Global Wheat Detection/'\n",
    "DIR_TRAIN = DIR + \"train\"\n",
    "DIR_TEST = DIR + \"test\"\n",
    "CHECKPOINT_DIR_PATH = '/content/gdrive/My Drive/Colab Notebooks/checkpoints/'\n",
    "\n",
    "### Loading Dataset\n",
    "df = pd.read_csv(DIR + \"train_pascall_voc_format.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oWFb2EQfFyPS"
   },
   "source": [
    "##Split data for K-fold (K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "sbA03xJ4CskY",
    "outputId": "20bad878-8b1f-4386-f75e-0107d4f6d2d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox_count</th>\n",
       "      <th>source</th>\n",
       "      <th>stratify_group</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00333207f</td>\n",
       "      <td>55</td>\n",
       "      <td>arvalis_1</td>\n",
       "      <td>arvalis_1_3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005b0d8bb</td>\n",
       "      <td>20</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>usask_1_1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006a994f7</td>\n",
       "      <td>25</td>\n",
       "      <td>inrae_1</td>\n",
       "      <td>inrae_1_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00764ad5d</td>\n",
       "      <td>41</td>\n",
       "      <td>inrae_1</td>\n",
       "      <td>inrae_1_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00b5fefed</td>\n",
       "      <td>25</td>\n",
       "      <td>arvalis_3</td>\n",
       "      <td>arvalis_3_1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>ffb445410</td>\n",
       "      <td>57</td>\n",
       "      <td>rres_1</td>\n",
       "      <td>rres_1_3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>ffbf75e5b</td>\n",
       "      <td>52</td>\n",
       "      <td>arvalis_1</td>\n",
       "      <td>arvalis_1_3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>ffbfe7cc0</td>\n",
       "      <td>34</td>\n",
       "      <td>arvalis_1</td>\n",
       "      <td>arvalis_1_2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>ffc870198</td>\n",
       "      <td>41</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>usask_1_2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>ffdf83e42</td>\n",
       "      <td>39</td>\n",
       "      <td>arvalis_1</td>\n",
       "      <td>arvalis_1_2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3368 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  bbox_count     source stratify_group  fold\n",
       "0     00333207f          55  arvalis_1    arvalis_1_3     1\n",
       "1     005b0d8bb          20    usask_1      usask_1_1     3\n",
       "2     006a994f7          25    inrae_1      inrae_1_1     1\n",
       "3     00764ad5d          41    inrae_1      inrae_1_2     0\n",
       "4     00b5fefed          25  arvalis_3    arvalis_3_1     3\n",
       "...         ...         ...        ...            ...   ...\n",
       "3363  ffb445410          57     rres_1       rres_1_3     1\n",
       "3364  ffbf75e5b          52  arvalis_1    arvalis_1_3     1\n",
       "3365  ffbfe7cc0          34  arvalis_1    arvalis_1_2     3\n",
       "3366  ffc870198          41    usask_1      usask_1_2     4\n",
       "3367  ffdf83e42          39  arvalis_1    arvalis_1_2     4\n",
       "\n",
       "[3368 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds = pd.read_csv(DIR + \"5_fold_new.csv\")\n",
    "df_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pwe4OxFikdOM"
   },
   "outputs": [],
   "source": [
    "def get_dataset_from_fold(df_folds, fold_number):\n",
    "    dataset = df_folds[df_folds['fold'] == fold_number]\n",
    "    images_list = dataset['image_id'].values  \n",
    "    dataset = df[df['image_id'].isin(images_list)]\n",
    "    return dataset\n",
    "\n",
    "def concat_dfs(df1, df2, df3, df4):\n",
    "    frames = [df1, df2, df3, df4]\n",
    "    return pd.concat(frames)\n",
    "\n",
    "\n",
    "# Spliting dataset into 5 folds\n",
    "mini_datasets = []\n",
    "for i in range(5):\n",
    "    dataset = get_dataset_from_fold(df_folds, i)\n",
    "    mini_datasets.append(dataset)\n",
    "\n",
    "kfold_split =[]\n",
    "for i, _ in enumerate(mini_datasets):\n",
    "    val = None\n",
    "    rest = []\n",
    "    for j, ds in enumerate(mini_datasets):\n",
    "        # print(ds)\n",
    "\n",
    "        if j == i:# validtion\n",
    "            # image_ids = ds['image_id'].unique()[:30]\n",
    "            # mini_dataset = ds[ds.image_id.isin(image_ids)]\n",
    "            # val = mini_dataset\n",
    "            val = ds\n",
    "        else:\n",
    "            # image_ids = ds['image_id'].unique()[:100]\n",
    "            # mini_dataset = ds[ds.image_id.isin(image_ids)]\n",
    "            # rest.append(mini_dataset)\n",
    "            rest.append(ds)\n",
    "    # print(rest)\n",
    "    kfold_split.append((pd.concat(rest), val))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3H7LmWuRk9rT",
    "outputId": "239a9948-17cf-46ae-dc53-eccc793084ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 2694 images, total bounding box: 117876\n",
      "Validition 674 images, total bounding box: 29791\n"
     ]
    }
   ],
   "source": [
    "### Splitting Train Dataset into train - val (80:20)\n",
    "## 'validation_index' represent which part of the data will use for validation from 'fold' column in the DataFrame\n",
    "VALIDATION_INDEX = 0 # Value in range 0-4\n",
    "\n",
    "train_df = kfold_split[VALIDATION_INDEX][0]\n",
    "valid_df = kfold_split[VALIDATION_INDEX][1]\n",
    "\n",
    "print(f'Train {len(train_df.image_id.unique())} images, total bounding box: {len(train_df)}')\n",
    "print(f'Validition {len(valid_df.image_id.unique())} images, total bounding box: {len(valid_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LlWnr_k-MYo6"
   },
   "source": [
    "# Image augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSbTcPTdHcP2"
   },
   "outputs": [],
   "source": [
    "# https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n",
    "#  https://towardsdatascience.com/automold-specialized-augmentation-library-for-autonomous-vehicles-1d085ed1f578\n",
    "#  http://albumentations-demo.herokuapp.com/\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Resize,\n",
    "    # color augmentaion:\n",
    "    RandomBrightness, RandomContrast,\n",
    "    RandomBrightnessContrast, ToGray, HueSaturationValue,\n",
    "\n",
    "    # wether augmentation:\n",
    "    # RandomFog, \n",
    "    RandomShadow, RandomRain,\n",
    "\n",
    "    # geomatric transforms:\n",
    "    HorizontalFlip, VerticalFlip, RandomRotate90,\n",
    "\n",
    "    # Blur:\n",
    "    Blur, MotionBlur, MedianBlur, \n",
    "\n",
    "    # noise and cut:\n",
    "    Cutout, \n",
    "    CoarseDropout,\n",
    "      MultiplicativeNoise      \n",
    ")\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "def get_train_transform():\n",
    "    return Compose([  \n",
    "                    HorizontalFlip(), \n",
    "                    VerticalFlip(), \n",
    "                    RandomRotate90(),   \n",
    "                    OneOf([                          \n",
    "                        RandomBrightness(), \n",
    "                        RandomContrast(),\n",
    "                        RandomBrightnessContrast(\n",
    "                            brightness_limit= 0.2,\n",
    "                            contrast_limit= 0.2),\n",
    "                        ToGray(),\n",
    "                        Blur(\n",
    "                            blur_limit=(3, np.random.choice([5, 7, 11, 13]))), \n",
    "                        MotionBlur(\n",
    "                            blur_limit=(3, np.random.choice([5, 7, 11, 13]))),\n",
    "                        MedianBlur(\n",
    "                            blur_limit=(3,5)),\n",
    "                           HueSaturationValue(\n",
    "                            hue_shift_limit = 0.2,\n",
    "                            sat_shift_limit = 0.2,\n",
    "                            val_shift_limit = 0.2\n",
    "                        )]),\n",
    "\n",
    "                    OneOf([\n",
    "                            RandomShadow(),\n",
    "                            RandomRain(rain_type=np.random.choice(['drizzle', 'heavy', 'torrential'])),\n",
    "                            CoarseDropout(\n",
    "                                    max_holes=30, \n",
    "                                    max_height=30, \n",
    "                                    max_width=30, \n",
    "                                    min_holes=5, \n",
    "                                    min_height=10, \n",
    "                                    min_width=10),\n",
    "                            Cutout(\n",
    "                                num_holes=30, \n",
    "                                max_h_size=30, \n",
    "                                max_w_size=30),\n",
    "                            MultiplicativeNoise()]),\n",
    "                    ToTensorV2(p=1.0)\n",
    "                   \n",
    "                    ],p=1., \n",
    "                    bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    \n",
    "def get_valid_transform():\n",
    "    return Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAH50bsSF_CA"
   },
   "source": [
    "## Wheat Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PBcb5YtagoC"
   },
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transforms=None, validation=False):\n",
    "        super().__init__()\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.validation = validation\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        # Retriving image id and records from df\n",
    "        image_id = self.image_ids[index]\n",
    "\n",
    "        # Activate cutmix only on half of the training Dataset\n",
    "        active_cutmix = np.random.uniform() > 0.5\n",
    "        if self.validation or active_cutmix:\n",
    "            image, boxes = self.load_image_and_boxes(index)\n",
    "        else:\n",
    "            image, boxes = self.load_cutmix_image_and_boxes(index)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['labels'] = labels\n",
    "                                        \n",
    "        # Applying Transforms\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "    \n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "        boxes = records[['x_start', 'y_start', 'x_end', 'y_end']].values\n",
    "        \n",
    "        return image, boxes\n",
    "\n",
    "    def load_cutmix_image_and_boxes(self, index, imsize=1024):\n",
    "        \"\"\" \n",
    "        This implementation of cutmix author:  https://www.kaggle.com/nvnnghia \n",
    "        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n",
    "        \"\"\"\n",
    "        w, h = imsize, imsize\n",
    "        s = imsize // 2\n",
    "        added_images = np.random.randint(1, 4) #images to use with cutmix\n",
    "        xc, yc = [int(np.random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n",
    "        indexes = [np.random.randint(0, self.image_ids.shape[0] - 1) for _ in range(added_images)]\n",
    "        for _ in range(4-added_images): \n",
    "            indexes += [index]\n",
    "        np.random.shuffle(indexes)\n",
    "        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n",
    "        result_boxes = []\n",
    "\n",
    "        for i, index in enumerate(indexes):\n",
    "            # i = np.random.randint(4)\n",
    "\n",
    "            image, boxes = self.load_image_and_boxes(index)\n",
    "            if i == 0: # top left\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
    "            elif i == 1:  # top right\n",
    "                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
    "                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "            elif i == 2:  # bottom left\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n",
    "            elif i == 3:  # bottom right\n",
    "                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "            padw = x1a - x1b\n",
    "            padh = y1a - y1b\n",
    "\n",
    "            boxes[:, 0] += padw\n",
    "            boxes[:, 1] += padh\n",
    "            boxes[:, 2] += padw\n",
    "            boxes[:, 3] += padh\n",
    "\n",
    "            result_boxes.append(boxes)\n",
    "\n",
    "        result_boxes = np.concatenate(result_boxes, 0)\n",
    "        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n",
    "        result_boxes = result_boxes.astype(np.int32)\n",
    "        result_boxes = result_boxes[np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)]\n",
    "        return result_image, result_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkk837Ki2y_G"
   },
   "outputs": [],
   "source": [
    "### Preparing Datasets and Dataloaders for Training \n",
    "train_dataset = WheatDataset(train_df, DIR_TRAIN, \n",
    "                             transforms = get_train_transform())\n",
    "valid_dataset = WheatDataset(valid_df, DIR_TRAIN, \n",
    "                             transforms = get_valid_transform(), validation=True)\n",
    "\n",
    "# DataLoaders\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 4,\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = 4,\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i60PAMSN6mWf"
   },
   "source": [
    "# Faster-R-CNN\n",
    "Faster R-CNN network is a single and unified network for object detection. It consists of the following two modules.\n",
    "\n",
    "\n",
    "\n",
    "1.   Deep fully convolutional network that proposes regions (RPN).\n",
    "2.   R-CNN detector that uses the proposed regions to a fully connected layer in order to classify any predict the bounding boxes for the image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![alt text](https://www.alegion.com/hs-fs/hubfs/1*uQNculABX73lqJwESVHr5w.png?width=1300&name=1*uQNculABX73lqJwESVHr5w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VzJnYQ2Kj-7S"
   },
   "source": [
    "# Why?\n",
    "In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the basis of several 1st-place entries in the tracks of Imagenet detection, Imagenet localization, COCO detection, and COCO segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24bKUq1lUlf5"
   },
   "source": [
    "#The Detector we built: FasterRCNN with ResNet backbone having pre-trained on ImageNet\n",
    "We use ResNet50, 101 and 152 as backbones followed by Feature Pyramid Network(FPN), and Region Proposal Network(RPN) with AnchorGenerator(scales=(32, 64, 128, 256, 512), ratios=(0.25, 0.5, 1, 2)) to produce Region of Intrests(RoI) filtered by Non Max Suppression(nms) with above 0.7 IoU threshold. After RoIAlign, the predictor predicts class score and bounding boxes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BA7h7nFLekg"
   },
   "source": [
    "# Why ResNet?\n",
    "ResNet, short of Residual Networks is a classic neural network used as a backbone for many computer vision tasks. This model was the winner of ImageNet challenge in 2015. The fundamental breakthrough with ResNet was it allowed us to train extremely deep neural networks with 150+layers successfully. Prior to ResNet training very deep neural networks was difficult due to the problem of vanishing gradients.\n",
    "\n",
    "As part of our research we took a look at the pytorch documentation and found their Faster R-CNN ResNet-50 FPN. We found as well that [\"residual networks can usually achieve better performance than most other backbones on object detection\"](https://www.researchgate.net/publication/333048961_Deep_Learning_Based_Fossil-Fuel_Power_Plant_Monitoring_in_High_Resolution_Remote_Sensing_Images_A_Comparative_Study) (page 8, last paragraph).\n",
    "\n",
    "With that information we wanted to use wider network to achieve better accuracy so, as part of our project, we used variuos backbones for cross-validation and further for ensemble.\n",
    "\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/gI4zT.png)\n",
    "\n",
    "\n",
    "For further info - https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33\n",
    "\n",
    "\n",
    "## Using deeper version of ResNet:\n",
    "One of the problems ResNets solve is the famous known vanishing gradient. This is because when the network is too deep, the gradients from where the loss function is calculated easily shrink to zero after several applications of the chain rule. This result on the weights never updating its values and therefore, no learning is being performed.\n",
    "With ResNets, the gradients can flow directly through the skip connections backwards from later layers to initial filters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MFTVucIGLwqu"
   },
   "source": [
    "\n",
    "## Feature Pyramid Network (FPN): \n",
    "![alt text](https://miro.medium.com/max/500/1*aMRoAN7CtD1gdzTaZIT5gA.png)\n",
    "\n",
    "A feature extractor designed for pyramid concept with accuracy and speed in mind. It replaces the feature extractor of detectors like Faster R-CNN and generates multiple feature map layers (multi-scale feature maps) with better quality information than the regular feature pyramid for object detection.\n",
    "\n",
    "FPN extracts feature maps and later feeds into a detector for object detection. RPN applies a sliding window over the feature maps to make predictions on the objectness (has an object or not) and the object boundary box at each location.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJ44IJVy9ZJN"
   },
   "source": [
    "# Region Proposal Network\n",
    "An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection.\n",
    "The RPN algorithm is based on the following steps:\n",
    "* Generate anchor boxes.\n",
    "* Classify each anchor box whether it is foreground or background.\n",
    "* Learn the shape offsets for anchor boxes to fit them for objects.\n",
    "\n",
    "Loss calculation: The RPN uses all the anchors selected for the mini batch to calculate the classification loss using **binary cross entropy**. Then, it uses only those minibatch anchors marked as foreground to calculate the regression loss. For calculating the targets for the regression, we use the foreground anchor and the closest ground truth object and calculate the correct Delta needed to transform the anchor into the object.\n",
    "For the regression error, the paper suggests using **Smooth L1 loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eK-5JS3M9M6E"
   },
   "source": [
    "\n",
    "## What is Anchor boxes:\n",
    "fixed sized reference bounding boxes which are placed uniformly throughout the original image. Instead of having to detect where objects are, we model the problem into two parts.\n",
    "For every anchor, we ask:\n",
    "*   Does this anchor contain a relevant object?\n",
    "*   How would we adjust this anchor to better fit the relevant object?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For details: [Torchvision finetuning tutorial](https://colab.research.google.com/github/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb)\n",
    "\n",
    "[Anchor boxes - key concept in Object detection](https://medium.com/@andersasac/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9)\n",
    "\n",
    "[Anchor boxes in FasterRCNN](https://medium.com/@medhijoydeep/anchor-boxes-in-faster-rcnn-6bd566ec4935)\n",
    "\n",
    "[FasterCNN with RPN](https://arxiv.org/abs/1506.01497)\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1484/1*oaQIpEtYpN1ZQbLhZhs47g.png)\n",
    "\n",
    "![alt text](https://user-images.githubusercontent.com/40360823/63683278-ece81c80-c834-11e9-8f36-d476532f8cd7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdAMCyzUhI--"
   },
   "source": [
    "#Second module - R-CNN:\n",
    "After having a list of possible relevant objects and their locations in the original image, it becomes a more straightforward problem to solve. Using the features extracted by the CNN and the bounding boxes with relevant objects, we apply Region of Interest (RoI) Pooling and extract those features which would correspond to the relevant objects into a new tensor.\n",
    "\n",
    "Finally, comes the R-CNN module, which uses that information to:\n",
    "\n",
    "* Classify the content in the bounding box (or discard it, using “background” as a label).\n",
    "* Adjust the bounding box coordinates (so it better fits the object).\n",
    "\n",
    "Loss caclulation: Following the same path as we did for the RPNs losses, the classification loss is now a **multiclass cross entropy loss**(binary in our case), using all the selected proposals and the **Smooth L1 loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZTXECCaM-4I"
   },
   "source": [
    "# Load model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkN8SBuErabp"
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor,FasterRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "\n",
    "\n",
    "def load_resnet(backbone_model): #backcone_model = ['resnet50', 'resnet101', 'resnet152']\n",
    "  num_classes = 2 # 1 class (wheat) + background\n",
    "  \n",
    "  # create an anchor_generator for the FPN\n",
    "  # which by default has 5 outputs\n",
    "  anchor_generator = AnchorGenerator(\n",
    "      sizes=tuple([(32, 64, 128, 256, 512) for _ in range(5)]),\n",
    "      aspect_ratios=tuple([(0.25, 0.5, 1.0, 2.0) for _ in range(5)]))\n",
    "  \n",
    "  pretrained_backbone=True\n",
    "  backbone = resnet_fpn_backbone(backbone_model, pretrained_backbone)\n",
    "  fasterrcnn = FasterRCNN(backbone, num_classes,\n",
    "                  rpn_anchor_generator=anchor_generator,\n",
    "                  rpn_head=RPNHead(256, anchor_generator.num_anchors_per_location()[0]))\n",
    "  in_features = fasterrcnn.roi_heads.box_predictor.cls_score.in_features\n",
    "  # replace the pre-trained head with a new one\n",
    "  fasterrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes = num_classes)\n",
    "  fasterrcnn.to(device)\n",
    "  return fasterrcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSMD6MUCRpgj"
   },
   "source": [
    "## Adjustments over the model:\n",
    "We took the [TORCHVISION OBJECT DETECTION FINETUNING TUTORIAL](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and made some adjustments to fit our model: \n",
    "1. By removing the defult scale of 16 - Our images size equals to 1024 pixels so we wanted to increase efficiency and reduce time computation.\n",
    "2. By setting the RPN head input to 256 (output of FPN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Av8E6V_A3ec"
   },
   "source": [
    "Note: in that precticular notebook we are showing the training proccess of arbitrary model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuqDohhosk1e"
   },
   "outputs": [],
   "source": [
    "backbones = ['resnet50', 'resnet101', 'resnet152']\n",
    "CHOSEN_BACKBONE = backbones[0] \n",
    "fasterrcnn = load_resnet(CHOSEN_BACKBONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kie1ZIETA6Pl"
   },
   "source": [
    "## Save & load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwOJjFcx2tNG"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR_PATH = '/content/gdrive/My Drive/Colab Notebooks/checkpoints/'\n",
    "\n",
    "def save_checkpoint(checkpoint_name, model, optimizer, lr_scheduler, loss, epoch):\n",
    "    path = CHECKPOINT_DIR_PATH + checkpoint_name \n",
    "    try:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_loss': loss['train_loss'],\n",
    "            'validation_loss': loss['validation_loss']\n",
    "            }, path)\n",
    "        print(f'Saved! {path}')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('NOT save!')\n",
    "\n",
    "def load_checkpoint(checkpoint_name, model, optimizer=None, lr_scheduler=None, warmup=False):\n",
    "    path = CHECKPOINT_DIR_PATH + checkpoint_name \n",
    "    print(path)\n",
    "    try:\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if not warmup:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "            train_loss = checkpoint['train_loss']\n",
    "            validation_loss = checkpoint['validation_loss']\n",
    "            epoch = checkpoint['epoch']\n",
    "            print('Load!')\n",
    "            return model, optimizer, lr_scheduler, train_loss, validation_loss, epoch\n",
    "        else:\n",
    "            print('Load!')\n",
    "            return model\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('NOT load!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8x27tZSt7ai"
   },
   "source": [
    "## Schedulers:\n",
    "A number of Schedulers were tested to accommodate some learning in training:\n",
    "\n",
    "StepLR - which requires a lot of time for good adjusting the scheduler hyperparameters.\n",
    "implemented cosine learing decay implementation from pytorch Schedulers which gave a different learning rate (compared to the optimizer learning rate and could not be used as mention in the git)\n",
    "manual cosine learing decay implementation which that solve the problem and achieved a moderate reduction in learning rate throughout training and evalotion learning rate values ​​as the optimizers\n",
    "\n",
    "[Cosine Learning rate decay](https://medium.com/@scorrea92/cosine-learning-rate-decay-e8b50aa455b)\n",
    "\n",
    "[Learning Rate Schedules and Adaptive Learning Rate Methods for Deep Learning](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)\n",
    "\n",
    "We used cosine class for setting the optimizer learning rate to be equal the scheduler learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akFOy08U2mAt"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "# https://github.com/pytorch/pytorch/issues/17913\n",
    "class LegacyCosineAnnealingLR(_LRScheduler):\n",
    "    r\"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
    "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n",
    "        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
    "\n",
    "    When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    It has been proposed in\n",
    "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n",
    "    implements the cosine annealing part of SGDR, and not the restarts.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        T_max (int): Maximum number of iterations.\n",
    "        eta_min (float): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "\n",
    "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
    "        https://arxiv.org/abs/1608.03983\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        super(LegacyCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\n",
    "                for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dEIJos7rSGjQ"
   },
   "source": [
    "# AdaMod\n",
    "AdaMod is a stochastic optimizer that restricts adaptive learning rates with adaptive and momental upper bounds. The dynamic learning rate bounds are based on the exponential moving averages of the adaptive learning rates themselves, which smooth out unexpected large learning rates and stabilize the training of deep neural networks.\n",
    "\n",
    "AdaMod is a drop in replacement for Adam. The only change is a new hyperparameter called B3, or Beta3. This controls the degree of lookback for the long term clipping average.\n",
    "\n",
    "sources:\n",
    " * [An Adaptive and Momental Bound Method for Stochastic Learning](https://arxiv.org/pdf/1910.12249.pdf)\n",
    "\n",
    " * [Meet AdaMod: a new deep learning optimizer with memory](https://medium.com/@lessw/meet-adamod-a-new-deep-learning-optimizer-with-memory-f01e831b80bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1eyEBgH04Ts"
   },
   "source": [
    "# Set model & fine tuning Hyper-parameters\n",
    "As we progressed, we used cross-validation on the hyper-parameters over 1-fold of our data. We notice that the optimal learning rate was 1e-4 with weight decay eqauls to 1e-4.\n",
    "\n",
    "In addition, We checked the minimum learning rate for our scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muS7qHh13teN"
   },
   "outputs": [],
   "source": [
    "### Preparing model for training\n",
    "\n",
    "#Num of epochs\n",
    "epochs = 40\n",
    "fasterrcnn.to(device)\n",
    "# Defininig Optimizer\n",
    "optimizer = adamod.AdaMod(fasterrcnn.parameters(), lr=0.0001, weight_decay=0.0001, beta3=0.999)\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = LegacyCosineAnnealingLR(optimizer, T_max=epochs)\n",
    "lr_scheduler.eta_min = 1.0000000000000002e-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zcafx-i18er"
   },
   "source": [
    "# Training part:\n",
    "For training part we used [Notebook](https://www.kaggle.com/jainamshah17/gwd-retinanet-pytorch-train) as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICmZIQTh3y-l"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_num, train_data_loader):\n",
    "    \n",
    "    print(\"Train Epoch - {} Started\".format(epoch_num))\n",
    "    st = time.time()\n",
    "    \n",
    "    fasterrcnn.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    total = 0\n",
    "    for iter_num, (images, targets, image_ids) in tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n",
    "                \n",
    "        # Reseting gradients after each iter\n",
    "        optimizer.zero_grad()\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward\n",
    "        \n",
    "        loss_dict = fasterrcnn(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += losses.item() * len(targets)\n",
    "        total += len(targets)\n",
    "\n",
    "        #Epoch Loss\n",
    "        \n",
    "        if iter_num % 80 == 0:\n",
    "            print(\n",
    "                'Epoch: {} | Iteration: {} | Running loss: {:1.5f}'.format(\n",
    "                    epoch_num, iter_num, running_loss / total))\n",
    "\n",
    "            \n",
    "    et = time.time()\n",
    "    print(\"\\n Total Time - {}\\n\".format(int(et - st)))\n",
    "    return running_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uxXYzCQ4CmW"
   },
   "outputs": [],
   "source": [
    "### One Epoch - Valid\n",
    "\n",
    "def valid_one_epoch(epoch_num, valid_data_loader):\n",
    "    \n",
    "    print(\"Validition Epoch - {} Started\".format(epoch_num))\n",
    "    st = time.time()\n",
    "    \n",
    "    epoch_loss = []\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    for iter_num, (images, targets, image_ids) in tqdm(enumerate(valid_data_loader), total=len(valid_data_loader)):\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward\n",
    "            loss_dict = fasterrcnn(images, targets)\n",
    "            \n",
    "            # Calculating Loss\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "           \n",
    "\n",
    "            running_loss += losses.item() * len(targets)\n",
    "            total += len(targets)\n",
    "\n",
    "            if iter_num %50 == 0:\n",
    "\n",
    "                print(\n",
    "                    'Epoch: {} | Iteration: {} | Running loss: {:1.5f}'.format(\n",
    "                        epoch_num, iter_num, running_loss/total))\n",
    "\n",
    "    et = time.time()\n",
    "    print(\"\\n Total Time - {}\\n\".format(int(et - st)))\n",
    "    return running_loss/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xilbLOr4nqG"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model_losses = {\n",
    "    'train_loss': [],\n",
    "    'validation_loss': []\n",
    "}\n",
    "train_loss, validition_loss = [], []\n",
    "\n",
    "epoches_range = range(epochs)\n",
    "\n",
    "file_name = 'Fasterrcnn - ' + CHOSEN_BACKBONE +  f' fold: {VALIDATION_INDEX}' + '.pt'\n",
    "load_file_path = file_name\n",
    "\n",
    "#Check if file exists\n",
    "if load_file_path:\n",
    "    fasterrcnn, _, _, train_loss, validation_loss,  = load_checkpoint(load_file_path, fasterrcnn, \n",
    "                                                                      optimizer, lr_scheduler)\n",
    "    [model_losses['train_loss'].append(l) for l in train_loss]\n",
    "    [model_losses['validation_loss'].append(l) for l in validation_loss ]\n",
    "    epoches_range = range((e+1), epochs, 1)\n",
    "    pprint(model_losses)\n",
    "\n",
    "\n",
    "for epoch in tqdm(epoches_range):\n",
    "    fasterrcnn.train()\n",
    "    tl, vl = 0, 0\n",
    "    \n",
    "    # Call train function\n",
    "    tl = train_one_epoch(epoch, train_data_loader)\n",
    "    model_losses['train_loss'].append(tl)\n",
    "    \n",
    "    # Call valid function\n",
    "    vl = valid_one_epoch(epoch, valid_data_loader)\n",
    "    model_losses['validation_loss'].append(vl)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('*'*30)\n",
    "    print(f'epoch: {epoch} learning rate {lr_scheduler.get_lr()[0]}\\n Train loss{tl}\\nvalidation loss {vl}')\n",
    "    print('*'*30)\n",
    "    print('\\n')\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    save_name = file_name\n",
    "    #save every 10 epochs\n",
    "    if epoch%10 == 0:\n",
    "        save_name = f' epoch: {epoch}' + save_name \n",
    "        results={'model_state_dict':fasterrcnn.state_dict(),\n",
    "         'model_losses':  model_losses}\n",
    "        torch.save(results, CHECKPOINT_DIR_PATH + save_name)\n",
    "    #Save checkpoint every epoch\n",
    "    save_checkpoint(save_name, \\\n",
    "                    fasterrcnn, optimizer,\\\n",
    "                    lr_scheduler,\\\n",
    "                    model_losses, epoch)\n",
    "results={'model_state_dict':fasterrcnn.state_dict(),\n",
    "         'model_losses':  model_losses}\n",
    "torch.save(results, CHECKPOINT_DIR_PATH + \"-final-\" + save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3FE5B1zxq0K"
   },
   "outputs": [],
   "source": [
    "def show_results(train, validiton, figsiz=[10, 20]):\n",
    "    X = len(train)\n",
    "    plt.figure(figsize=figsiz)\n",
    "    Epoches = np.linspace(1, X, X, dtype=np.int32)\n",
    "\n",
    "    plt.plot(Epoches, train, label='train')\n",
    "    plt.plot(Epoches, validiton, label='validition')\n",
    "    plt.title(\"A Losses Curves\", fontsize=24)\n",
    "    plt.xlabel(\"Epoches \", fontsize=24)\n",
    "    plt.ylabel(\"Loss\", fontsize=24)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "show_results(model_losses['train_loss'], \n",
    "            model_losses['validation_loss'])  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MASTER  Notebook wheat_detection_FasterRCNN_resnet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
